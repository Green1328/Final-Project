import pandas as pd
import numpy as np
import tensorflow as tf

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

#Load the dataset
data = pd.read_csv('./Maternal Health Risk Data Set.csv')
data.describe()
data.info()

#Preprocess the data
#Need to convert RiskLevel to binary 
print("Original unique values in RiskLevel:", data['RiskLevel'].unique())

data['RiskLevel'] = data['RiskLevel'].astype(str)# Convert RiskLevel to string 
data['RiskLevel'] = data['RiskLevel'].map({'low risk': 0, 'high risk': 1, 'mid risk': 1})

# Check for missing values and unique values after mapping
print("Missing values after mapping:", data['RiskLevel'].isnull().sum())
print("Unique values in RiskLevel after mapping:", data['RiskLevel'].unique())

# VIew first few rows
print(data['RiskLevel'].head())

data['RiskLevel'] = data['RiskLevel'].fillna(0) #empty value, converts to 0

X= data.drop('RiskLevel', axis =1)
y= data['RiskLevel']


##Performance Metrics
# Use confusion_matrix lib to get TP,TN,FP,FN
def calculate_metrics(y_true, y_pred):
    TN, FP,FN, TP =confusion_matrix(y_true, y_pred).ravel()
    P= TP+FN
    N= TN+FP
    precision = TP/(TP+FP) #quality of the positive prediction
    TNR = TN/(TN+FP) #or specificity
    TPR = TP / (TP+FN) #sensitivity or recall(r)
    FPR = FP/(TN+FP) #False positive rate
    FNR = FN/(TP+FN) #False negative rate 
    f1_measure = 2*(precision*TPR)/(precision+TPR)
    accuracy= (TP+TN)/(P+N) #accuracy
    error_rate= (FP+FN)/(P+N) #error rate
    BACC=(TPR+TNR)/2 #Balanced accuracy:measures average sensitive and specificity
    TSS = TPR-FPR #True Skill Statistics: measures the difference b/w recall minus probability of false detection
    HSS = (2*(TP*TN - FP*FN))/((TP+FN)*(FN+TN)+(TP+FP)*(TN+FP)) #Heidke skill score: measures the fractional prediction over random predication
    return  [TP, TN, FP, FN, TNR, TPR, FPR, FNR, f1_measure,precision,error_rate,accuracy, BACC, TSS,HSS]

def evaluate_model(model,X_train,X_test,y_train, y_test, is_lstm=False):
    if is_lstm:
        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],1))
        X_test=X_test.reshape((X_test.shape[0], X_test.shape[1],1))
        #train model
        model.fit(X_train,y_train,epochs=10,batch_size=32,verbose=0)
        predict_y=(model.predict(X_test)>0.5).astype(int).flatten()             
    else:
        model.fit(X_train,y_train) #train non-lstm model
        predict_y=model.predict(X_test)

    metrics =calculate_metrics(y_test,predict_y)
    return metrics


##10-fold cross validation method
kf = KFold(n_splits=10, shuffle=True, random_state=42)
results = {'Random Forest': [] , 'KNN': [], 'LSTM': []}
fold_results=[]
#k-fold for Random forest
for fold, (train_index, test_index) in enumerate(kf.split(X), start=1):
    print(f"\nMetrics for all ALgorithms in Iteration {fold}:")
    # Splitting the data for each fold
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    #standarize data for each fold
    scaler= StandardScaler()
    X_train_stan=scaler.fit_transform(X_train)
    X_test_stan=scaler.transform(X_test)    


    ##Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_metrics= evaluate_model(rf,X_train_stan,X_test_stan,y_train,y_test)
    

    ##KNN
    knn = KNeighborsClassifier(n_neighbors=5)
    knn_metrics = evaluate_model(knn, X_train_stan,X_test_stan,y_train,y_test)

    ##LSTM
    model =Sequential()
    model.add(LSTM(50, activation='relu', input_shape = (X_train_stan.shape[1], 1)))
    model.add(Dense(1, activation ='sigmoid'))
    model.compile(optimizer ='adam', loss ='binary_crossentropy', metrics = ['accuracy'])
    lstm_metrics=evaluate_model(model,X_train_stan,X_test_stan, y_train, y_test, is_lstm=True)

    fold_results.append({   #store results for fold
        'KNN':knn_metrics,
        'RF': rf_metrics,
        'LSTM': lstm_metrics,
    })

    metrics_table=pd.DataFrame({
        'Metrics': [
            'TP', 'TN', 'FP', 'FN', 'TNR', 'TPR', 'FPR', 'FNR', 'f1_measure','precision','error_rate','accuracy', 'BACC', 'TSS','HSS'
        ],
        'KNN':knn_metrics,
        'RF': rf_metrics,
        'LSTM': lstm_metrics
    })

    print(metrics_table.to_string(index=False))




avg_metrics = {'KNN': [], 'RF':[], 'LSTM':[]}

for model_name in avg_metrics.keys():
    all_metrics = [fold_result[model_name] for fold_result in fold_results]
    all_metrics_np = np.array(all_metrics)
    avg_metrics[model_name]=np.mean(all_metrics_np, axis=0)

print("\nAverage metrics for each model:")
metrics_avg_df=pd.DataFrame(avg_metrics)
metrics_avg_df['Metrics'] = ['TP', 'TN', 'FP', 'FN', 'TNR', 'TPR', 'FPR', 'FNR', 'f1_measure','precision','error_rate','accuracy', 'BACC', 'TSS','HSS'
]
print(metrics_avg_df.to_string(index=False))
   
